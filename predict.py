import os
import torch
import transformers
import pandas as pd
import numpy as np
import joblib
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef, classification_report

class BERTLogisticEvaluator:
    """
    BERT+é€»è¾‘å›å½’æ¨¡å‹è¯„ä¼°å™¨
    æ”¯æŒCoLAï¼ˆè¯­è¨€å¯æ¥å—æ€§ï¼‰å’ŒSST-2ï¼ˆæƒ…æ„Ÿåˆ†æï¼‰ä¸¤ä¸ªä»»åŠ¡
    å¯ä»¥å¯¹éªŒè¯é›†è¿›è¡Œè¯„ä¼°ï¼Œä¹Ÿå¯ä»¥å¯¹æ— æ ‡ç­¾æ•°æ®è¿›è¡Œé¢„æµ‹
    """
    def __init__(self, task='cola', model_path='./bert_model'):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        Args:
            task: ä»»åŠ¡åç§°ï¼Œå¯é€‰'cola'æˆ–'sst2'
            model_path: BERTé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        """
        self.task = task.lower()
        self.model_path = model_path
        # æ ¹æ®ä»»åŠ¡åç§°ç¡®å®šä¿å­˜çš„é€»è¾‘å›å½’æ¨¡å‹è·¯å¾„
        self.saved_model_path = f"{self.task}_lr_model/model_params.joblib"
        # è®¾ç½®è®¡ç®—è®¾å¤‡ï¼ˆGPU/CPUï¼‰
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # åˆå§‹åŒ–Sparkä¼šè¯
        self.spark = SparkSession.builder \
            .appName(f"{self.task.upper()}_Evaluate") \
            .master("local[*]") \
            .config("spark.driver.memory", "4g") \
            .getOrCreate()
        # è®¾ç½®Sparkæ—¥å¿—çº§åˆ«
        self.spark.sparkContext.setLogLevel("ERROR")

        # æ£€æŸ¥å¹¶åŠ è½½BERTæ¨¡å‹
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"BERTæ¨¡å‹æœªæ‰¾åˆ°: {model_path}")
        print(f"åŠ è½½ BERT æ¨¡å‹ä¸ Tokenizer from {model_path}...")
        self.tokenizer = transformers.BertTokenizer.from_pretrained(model_path)
        self.bert = transformers.BertModel.from_pretrained(model_path, output_hidden_states=True).to(self.device)
        self.bert.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

        # æ£€æŸ¥å¹¶åŠ è½½è®­ç»ƒå¥½çš„é€»è¾‘å›å½’æ¨¡å‹å‚æ•°
        if not os.path.exists(self.saved_model_path):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ä¿å­˜çš„Logisticå›å½’æ¨¡å‹å‚æ•°: {self.saved_model_path}")
        self.lr_params = joblib.load(self.saved_model_path)
        print(f"ä»»åŠ¡: {self.task.upper()} | ä½¿ç”¨è®¾å¤‡: {self.device}")

    def extract_features(self, sentences, batch_size=32):
        """
        ä½¿ç”¨BERTæ¨¡å‹æå–æ–‡æœ¬ç‰¹å¾
        Args:
            sentences: å¾…å¤„ç†çš„å¥å­åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤32
        Returns:
            final_feats: åŒ…å«BERTç‰¹å¾å’Œå¥å­é•¿åº¦çš„numpyæ•°ç»„
        """
        features, lengths = [], []
        with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜
            for i in range(0, len(sentences), batch_size):
                batch = sentences[i:i + batch_size]
                # å¯¹æ‰¹æ¬¡æ•°æ®è¿›è¡Œåˆ†è¯å’Œç¼–ç 
                tokens = self.tokenizer(batch, padding=True, truncation=True, return_tensors="pt")
                input_ids = tokens['input_ids'].to(self.device)
                attention_mask = tokens['attention_mask'].to(self.device)
                # è·å–BERTè¾“å‡º
                outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
                # æå–[CLS]æ ‡è®°çš„è¾“å‡ºä½œä¸ºå¥å­è¡¨ç¤º
                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                features.append(cls_embeddings)
                # è®¡ç®—æ¯ä¸ªå¥å­çš„è¯æ•°
                lengths.extend([len(s.split()) for s in batch])
        # å°†BERTç‰¹å¾å’Œå¥å­é•¿åº¦ç‰¹å¾æ‹¼æ¥
        final_feats = np.hstack([np.vstack(features), np.array(lengths).reshape(-1, 1)])
        return final_feats

    def load_labeled_dataset(self, path):
        """
        åŠ è½½å¸¦æ ‡ç­¾çš„æ•°æ®é›†ï¼ˆç”¨äºè¯„ä¼°ï¼‰
        Args:
            path: æ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼ˆTSVæ ¼å¼ï¼‰
        Returns:
            DataFrame: åŒ…å«å¥å­å’Œæ ‡ç­¾çš„Pandas DataFrame
        """
        schema = StructType([
            StructField("sentence", StringType(), True),
            StructField("label", IntegerType(), True)
        ])
        return self.spark.read.csv(path, schema=schema, sep='\t', header=True).toPandas()

    def load_unlabeled_dataset(self, path):
        """
        åŠ è½½æ— æ ‡ç­¾çš„æ•°æ®é›†ï¼ˆç”¨äºé¢„æµ‹ï¼‰
        Args:
            path: æ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼ˆTSVæ ¼å¼ï¼‰
        Returns:
            DataFrame: åªåŒ…å«å¥å­çš„Pandas DataFrame
        """
        schema = StructType([
            StructField("sentence", StringType(), True)
        ])
        return self.spark.read.csv(path, schema=schema, sep='\t', header=True).toPandas()

    def predict(self, sentences):
        """
        å¯¹è¾“å…¥å¥å­è¿›è¡Œé¢„æµ‹
        Args:
            sentences: å¾…é¢„æµ‹çš„å¥å­åˆ—è¡¨
        Returns:
            preds: é¢„æµ‹æ ‡ç­¾æ•°ç»„
        """
        # æå–ç‰¹å¾
        X = self.extract_features(sentences)
        # ä½¿ç”¨ä¿å­˜çš„å‚æ•°é‡å»ºé€»è¾‘å›å½’æ¨¡å‹
        clf = LogisticRegression()
        clf.coef_ = self.lr_params['coefficients'].reshape(1, -1)
        clf.intercept_ = np.array([self.lr_params['intercept']])
        clf.classes_ = np.array([0, 1])
        # è¿›è¡Œé¢„æµ‹
        preds = clf.predict(X)
        return preds

    def evaluate(self, val_path):
        """
        åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½
        Args:
            val_path: éªŒè¯é›†æ–‡ä»¶è·¯å¾„
        """
        print(f"åŠ è½½éªŒè¯é›†: {val_path}")
        df = self.load_labeled_dataset(val_path)
        y_true = df['label'].values
        sentences = df['sentence'].tolist()
        y_pred = self.predict(sentences)

        # è®¡ç®—å¹¶è¾“å‡ºå¤šä¸ªè¯„ä¼°æŒ‡æ ‡
        print("\nğŸ“Š éªŒè¯é›†è¯„ä¼°æŒ‡æ ‡:")
        print(f"å‡†ç¡®ç‡: {accuracy_score(y_true, y_pred):.4f}")
        print(f"ç²¾ç¡®ç‡: {precision_score(y_true, y_pred):.4f}")
        print(f"å¬å›ç‡: {recall_score(y_true, y_pred):.4f}")
        print(f"F1 åˆ†æ•°: {f1_score(y_true, y_pred):.4f}")
        print(f"MCC: {matthews_corrcoef(y_true, y_pred):.4f}")
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_true, y_pred, digits=4))

    def predict_and_save(self, text_path, output_path):
        """
        å¯¹æ— æ ‡ç­¾æ•°æ®è¿›è¡Œé¢„æµ‹å¹¶ä¿å­˜ç»“æœ
        Args:
            text_path: è¾“å…¥æ–‡æœ¬æ–‡ä»¶è·¯å¾„
            output_path: é¢„æµ‹ç»“æœä¿å­˜è·¯å¾„
        """
        print(f"åŠ è½½å¾…é¢„æµ‹æ–‡æœ¬: {text_path}")
        df = self.load_unlabeled_dataset(text_path)
        sentences = df['sentence'].tolist()
        # è¿›è¡Œé¢„æµ‹
        preds = self.predict(sentences)
        # å°†é¢„æµ‹ç»“æœæ·»åŠ åˆ°DataFrameå¹¶ä¿å­˜
        df['prediction'] = preds
        df.to_csv(output_path, sep='\t', index=False)
        print(f"é¢„æµ‹ç»“æœä¿å­˜åˆ°: {output_path}")

def main():
    """
    ä¸»å‡½æ•°ï¼Œè®¾ç½®ä»»åŠ¡å‚æ•°å¹¶è¿è¡Œè¯„ä¼°å’Œé¢„æµ‹
    æ”¯æŒCoLAå’ŒSST-2ä¸¤ä¸ªä»»åŠ¡ï¼Œå¯é€šè¿‡ä¿®æ”¹taskå‚æ•°åˆ‡æ¢
    """
    task = 'sst2'  # å¯æ”¹ä¸º 'sst2'
    model_path = './bert_model'  # BERTæ¨¡å‹è·¯å¾„
    val_path = f'processed_data/{task}_dev.tsv'  # éªŒè¯é›†è·¯å¾„ï¼ˆå¸¦æ ‡ç­¾ï¼‰
    text_path = f'processed_data/{task}_test.tsv'  # æµ‹è¯•é›†è·¯å¾„ï¼ˆæ— æ ‡ç­¾ï¼‰
    pred_output = f'processed_data/{task}_test_pred.tsv'  # é¢„æµ‹ç»“æœè¾“å‡ºè·¯å¾„

    # åˆ›å»ºè¯„ä¼°å™¨å®ä¾‹
    evaluator = BERTLogisticEvaluator(task=task, model_path=model_path)
    evaluator.evaluate(val_path)  # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½
    evaluator.predict_and_save(text_path, pred_output)  # å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹å¹¶ä¿å­˜ç»“æœ
    # åœæ­¢Sparkä¼šè¯ï¼Œé¿å…è¿æ¥é”™è¯¯
    evaluator.spark.stop()

if __name__ == "__main__":
    main()
